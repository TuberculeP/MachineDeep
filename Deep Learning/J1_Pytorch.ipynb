{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in ./.venv/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: graphviz in ./.venv/lib/python3.11/site-packages (0.20.3)\n",
      "Requirement already satisfied: onnx in ./.venv/lib/python3.11/site-packages (1.17.0)\n",
      "Requirement already satisfied: onnxscript in ./.venv/lib/python3.11/site-packages (0.1.0)\n",
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.20.1-cp311-cp311-macosx_13_0_universal2.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.11/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (4.55.6)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in ./.venv/lib/python3.11/site-packages (from onnx) (5.29.3)\n",
      "Requirement already satisfied: ml_dtypes in ./.venv/lib/python3.11/site-packages (from onnxscript) (0.5.1)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime)\n",
      "  Downloading flatbuffers-25.1.24-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading onnxruntime-1.20.1-cp311-cp311-macosx_13_0_universal2.whl (31.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-25.1.24-py2.py3-none-any.whl (30 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Installing collected packages: flatbuffers, humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 flatbuffers-25.1.24 humanfriendly-10.0 onnxruntime-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install torch torchvision torchaudio matplotlib pandas graphviz onnx onnxscript onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer des données : un dataset préfait (MNIST)\n",
    "\n",
    "training_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset)\n",
    "print(training_dataset.data.shape)\n",
    "print(training_dataset.targets.shape)\n",
    "print(training_dataset.targets.unique())\n",
    "print(training_dataset.targets.bincount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3294, 0.7255,\n",
       "           0.6235, 0.5922, 0.2353, 0.1412, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8706, 0.9961,\n",
       "           0.9961, 0.9961, 0.9961, 0.9451, 0.7765, 0.7765, 0.7765, 0.7765,\n",
       "           0.7765, 0.7765, 0.7765, 0.7765, 0.6667, 0.2039, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.4471,\n",
       "           0.2824, 0.4471, 0.6392, 0.8902, 0.9961, 0.8824, 0.9961, 0.9961,\n",
       "           0.9961, 0.9804, 0.8980, 0.9961, 0.9961, 0.5490, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0667, 0.2588, 0.0549, 0.2627, 0.2627,\n",
       "           0.2627, 0.2314, 0.0824, 0.9255, 0.9961, 0.4157, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.3255, 0.9922, 0.8196, 0.0706, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0863, 0.9137, 1.0000, 0.3255, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.5059, 0.9961, 0.9333, 0.1725, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.2314, 0.9765, 0.9961, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.5216, 0.9961, 0.7333, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353,\n",
       "           0.8039, 0.9725, 0.2275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4941,\n",
       "           0.9961, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2941, 0.9843,\n",
       "           0.9412, 0.2235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.8667, 0.9961,\n",
       "           0.6510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.7961, 0.9961, 0.8588,\n",
       "           0.1373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1490, 0.9961, 0.9961, 0.3020,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.1216, 0.8784, 0.9961, 0.4510, 0.0039,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5216, 0.9961, 0.9961, 0.2039, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.2392, 0.9490, 0.9961, 0.9961, 0.2039, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.4745, 0.9961, 0.9961, 0.8588, 0.1569, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.4745, 0.9961, 0.8118, 0.0706, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 7)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn4AAAKSCAYAAABMVtaZAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAALFpJREFUeJzt3Qm0ldV5P+B9FRExGmctCFERMQ5xHsAYQbTqcqktKg5xOdQpiUMU1CRWRLTqMio41qFVlIqlonVK1QpG0AiOmZCoOIEMy4miMihc8PzXuf82TZNvIwfPPefe8z7PWi713b7ft5H7wY99z95fU6lUKiUAABreKvWeAAAAtSH4AQAEIfgBAAQh+AEABCH4AQAEIfgBAAQh+AEABCH4AQAEIfgBAAQh+AEABCH4tQEvvfRSOvPMM9O2226b1lxzzdS9e/c0cODANG3atHpPDRrG1KlT05FHHpm22GKL1Llz57TBBhuk733ve+nRRx+t99Sg4SxYsCANHTo0HXjggWm99dZLTU1N6a677qr3tEgpdaj3BEjpqquuSs8991zLb0rf+c530vvvv59uuummtPPOO6fnn38+bbfddvWeIrR7M2bMSPPnz08nnHBC6tKlS1q0aFF64IEH0qGHHppuu+22dNppp9V7itAwPv7443TppZe2LGTssMMOacKECfWeEv+tqVQqlf7nX6iPSZMmpV133TV17Njxj7U333wzbb/99umII45I99xzT13nB41q2bJlaZdddklffPFFev311+s9HWgYixcvTvPmzUubbLJJevnll9Nuu+2WRo4cmU488cR6Ty083+ptA/r06fN/Ql9Zz549W771+9prr9VtXtDoVl111dStW7f0ySef1Hsq0FBWX331ltBH2+NbvW1UeSH2gw8+aAl/QPUsXLgwff755+nTTz9NjzzySHr88cfTUUcdVe9pAdSE4NdGjR49Os2ePbvlMxJA9QwePLjlM31lq6yyShowYEDLZ2oBIhD82qDyZ43OOOOM1Lt375YPogPVc84557R8dnbOnDnpvvvua/mc35IlS+o9LYCa8Bm/Nqa8o/fggw9O3/zmN9P999/f8hkkoHq23nrrtN9++6Xjjz8+/eIXv2g5duKQQw5p+XgFQKMT/NqQ8meODjrooJYPmj/xxBMtR04Arau8+lc+S9O5mUAEvtXbRpSPkyivOpR/8xk/fnzaZptt6j0lCKG80eN//uAF0Ois+LUB5c8YlXcVTp48OY0dO7bls31AdX344Yd/UWtubk6jRo1Ka6yxhj9sASFY8WsjuwzLx0qUV/z+67/+6y8ObD7uuOPqNjdoFKeffnr67LPPWl7T1rVr15bP05Z3z5c3U1177bXpG9/4Rr2nCA2lvFu+/NGl8kaqsvLrEWfNmtXyz2eddVbLZ9mpPW/uaAP69u2bJk6cmB33UwRf35gxY9Idd9yRpkyZkubOnZvWWmutlrd2lH8DKr+2DaiuzTbbrOVViUXefffdlnFqT/ADAAjCZ/wAAIIQ/AAAghD8AACCEPwAAIIQ/AAAghD8AACCEPwAAIJY4Td3NDU1te5MoA7a4jGWnjUakWcN2sazZsUPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgiA71nkBb07dv34rHhg4dmu2ZMGFCYX3ixIkVzy13ra8ag7Zov/32y449+eSTVbvP4sWLs2OXXnppYf3KK6+s2v2h0QwZMqSi56ls3333Law//fTTVZsXK8aKHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQTaVSqbRC/2FTU2okK/jDbveGDRuWHbvkkktSdG3x66DRnrUePXoU1h977LFsz5ZbbpnqaeHChdmxww8/vLA+bty4VpxR++dZa186d+6cHXv77bcL6xtttFG259FHHy2sDxw4MNuzZMmS5c6RlXvWrPgBAAQh+AEABCH4AQAEIfgBAAQh+AEABCH4AQAEEfY4l9xRJvvss0+2Z+LEiamtGjp0aNWu1a9fv+zYhAkTUiNxxETr22WXXQrrjz/+eLZn/fXXT23152DBggWF9aOOOirbs7wfaxSetfbl6quvzo4NGjSoavc54ogjsmMPPvhg1e4TSclxLgAAlAl+AABBCH4AAEEIfgAAQQh+AABBhN3VG8XTTz+dHevbt2/F12u0rwM7DevnzjvvzI6dcMIJhfUzzzwz2/PLX/6ysN6pU6dsz5gxYwrrvXr1qvhrZtSoUdmek046KUXnWWtfvvWtb2XHXn755cL6euutV/F9pkyZkh3bcccdK74edvUCAPDfBD8AgCAEPwCAIAQ/AIAgBD8AgCAEPwCAIDrUewJATOecc0527B//8R8L62+//Xa2Z968eYX1Dh3yv8z9+te/rvg4F4hgxowZ2bHm5uaq3adnz55VuxYrxoofAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBB29Ta4vn371nsKUOizzz6r+CXwy9OxY8fC+rBhw7I9Rx99dKqWWbNmVe1aAK3Fih8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQjnNpEE8//XTVrtWvX7+qXQt69OhRWF9ttdWyPSeffHJF1yrr1KlTYf2AAw5I1fTYY48V1i+//PKq3gegNVjxAwAIQvADAAhC8AMACELwAwAIQvADAAjCrt4G2bnbt2/fqt1nwoQJVbsWPProo4X1Xr16pfYot7N4s802y/a8/fbbhfXm5uaqzQvao1VWya8/bb311oX1119/vRVn1Pis+AEABCH4AQAEIfgBAAQh+AEABCH4AQAEIfgBAAThOJd2dGxLNY9sKRs2bFhVrwcR5I6hmTp1arbn3nvvLaxfdtll2Z5p06atxOygfenYsWN2bNCgQYX10047rRVn1Pis+AEABCH4AQAEIfgBAAQh+AEABCH4AQAEYVdvnSxvh241d+8ub+fuJZdcUrX7AHnHHntsYX2vvfbK9lx99dWF9VtuuaVq8wLiseIHABCE4AcAEITgBwAQhOAHABCE4AcAEITgBwAQhONcWlnuaJann366qveZMGFCYd2RLdTbP/3TPxXWN9xww5rc/5FHHsmOrbPOOoX1733ve9mewYMHF9ZXW221iue22WabZcdOPPHEwrrjXKi3JUuW1HsKfA1W/AAAghD8AACCEPwAAIIQ/AAAghD8AACCsKu3Trt6q7lzt6xfv35Vuw9U04gRI1J788QTT2THcjvyH3jggWzPmmuuWfEcdt5558L6ZZddlu0ZMmRIxfeBSp100kmF9fHjx9d8LlTOih8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQjnNpZUOHDq3atYYNG1a1awErZ9y4cYX1Cy+8MNtz/fXXV3yfVVYp/nP5gAEDsj2OcwG+ihU/AIAgBD8AgCAEPwCAIAQ/AIAgBD8AgCDs6q2CUqlU1evldu9OmDChqvcBquezzz6r9xQAvpIVPwCAIAQ/AIAgBD8AgCAEPwCAIAQ/AIAgBD8AgCAc51KBSy65pKHuA1RPjx49anKft956qyb3ARqTFT8AgCAEPwCAIAQ/AIAgBD8AgCAEPwCAIOzq/TN9+/bNjg0dOrRq9+nXr1/VrgVt2ciRIwvrm266acXX+uijj7JjP//5zwvrU6ZMyfbssssuFc8h9+vAvvvum6pp4cKFhfXhw4dX9T5QqXfffbew/sorr1T1Wcv1bLHFFtmed955p+L7RGPFDwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIAjHufyZp59+uqrXGzZsWGF9woQJKcoxOI32Y6Uye+yxR2G9V69eVb3PYYcdVlh/6qmnsj0HH3xwYX2VVfJ/Jv7yyy9TLUydOrWwPnHixJrcH3KmT59eWH/wwQezPTvttFPFz9qOO+5YWN9vv/2yPbfffnt2jP/Pih8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEHb1VrADdXk7V2vhkksuqer1ci+bX5ldyvvss0+2x65eaqFTp04V7dxtCz766KPs2OjRo2s6F/i6rrzyyuxYt27dCuunn356xfcZMmRIdsyu3q9mxQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIx7lU8AL0lTnOJXdkysocpVJtuWNWlvf/oNpHytD4Fi9enCJYsGBBdmz48OGF9VtuuSXb8+GHH1ZlXtAWPPXUU1U7zoWvx4ofAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBBNpVKptEL/YVNT68+mnVreTtdq7t4dNmxYxT124S7fCn7511SjPWtbbbVVYX3AgAGp3v76r/+6sD5u3LiKv2ZuvvnmbM/8+fNTdJ612Lp161ZY33fffbM9d955Z2F9zpw5Fd8nktJXPGtW/AAAghD8AACCEPwAAIIQ/AAAghD8AACCEPwAAIJwnAuhOWICasOzBrXhOBcAAFoIfgAAQQh+AABBCH4AAEEIfgAAQQh+AABBCH4AAEEIfgAAQQh+AABBCH4AAEEIfgAAQQh+AABBCH4AAEEIfgAAQQh+AABBCH4AAEEIfgAAQQh+AABBCH4AAEEIfgAAQQh+AABBCH4AAEEIfgAAQQh+AABBCH4AAEE0lUqlUr0nAQBA67PiBwAQhOAHABCE4AcAEITgBwAQhOAHABCE4AcAEITgBwAQhOAHABCE4AcAEITgBwAQhOAHABCE4AcAEITgBwAQhOAHABCE4NdG/PrXv06HHnpoWm+99VLnzp3Tdtttl2644YZ6Twsa2uWXX56amppanjeg9XjW2o4O9Z4AKT355JPpkEMOSTvttFMaMmRI+sY3vpHefvvtNGvWrHpPDRpW+fm64oor0pprrlnvqUBD86y1LU2lUqlU70lE9tlnn6Wtttoq9enTJ91///1plVUswkItHH300emjjz5Ky5YtSx9//HF69dVX6z0laEietbZFyqize++9N33wwQcty+Dl0Ldw4cL05Zdf1nta0NCeeeaZlj9oXXfddfWeCjQ0z1rbI/jV2fjx49Paa6+dZs+enXr16tXybd7yv//whz9MX3zxRb2nBw2nvOpw1llnpVNOOSVtv/329Z4ONCzPWtvkM3519uabb6alS5emww47LJ188snpyiuvTBMmTEg33nhj+uSTT9K//uu/1nuK0FBuvfXWNGPGjJY/dAGtx7PWNgl+dbZgwYK0aNGi9IMf/OCPu3gHDBiQlixZkm677bZ06aWXpp49e9Z7mtAQ5s6dmy6++OKWTVQbbrhhvacDDcuz1nb5Vm+drbHGGi1/P+aYY/5P/dhjj235++TJk+syL2hEF110UcuRSeVvPwGtx7PWdlnxq7MuXbqkqVOnpo033vj/1DfaaKOWv8+bN69OM4PG+1jF7bff3vIh8zlz5vyxXv4sbXNzc5o+fXrL52vLv1kBK8+z1rZZ8auzXXbZpeXv5c0df+p/HhZL5FAd5WesvGP+7LPPTptvvvkf/3rhhRfStGnTWv65/NEK4OvxrLVtzvGrs9/85jdp5513bvnW7ujRo/9YL//72LFjWz4YW14VBL6e8vlhv/rVrwq/JTV//vx0/fXXpx49eth9CF+TZ61tE/zagPJu3jvvvDMNHDgw7bPPPi27esuh72c/+1nLaedA6+nbt69DZaEGPGttg8/4tZEt7927d08jR45MDz74YPrWt76VRowYkc4555x6Tw0AaCBW/AAAgrC5AwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgiBU+wLmpqal1ZwJ10BaPsfSs0Yg8a9A2njUrfgAAQQh+AABBCH4AAEEIfgAAQQh+AABBCH4AAEEIfgAAQQh+AABBCH4AAEEIfgAAQQh+AABBCH4AAEF0qPcEGkHHjh2zYwMHDiysd+rUKdvzz//8z1WZFwDAn7LiBwAQhOAHABCE4AcAEITgBwAQhOAHABCEXb0V6NGjR2F91KhR2Z4tt9yysP7oo49me+zqBQBagxU/AIAgBD8AgCAEPwCAIAQ/AIAgBD8AgCAEPwCAIBznUoFTTz21sL7nnntWfK158+ZVYUYAACvOih8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEE2lUqm0Qv9hU1OKYLPNNsuOTZo0qbC+8cYbV3yfXr16Zcfeeuutiq/HylnBL/+aivKsEYtnDdrGs2bFDwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIIgO9Z5AW7POOutkx1bm2JZnn322sD5nzpyKrwVU18iRIwvrc+fOzfacd955qa068cQTC+uXXHJJtufmm28urF999dVVmxfQdljxAwAIQvADAAhC8AMACELwAwAIQvADAAgi7K7eNddcs7B+/vnnV/U+11xzTWF90aJFVb0PRLfuuusW1seMGZPt6devX2H9oYceSvXWqVOnwvo//MM/ZHvOOOOMwnrHjh2zPQ888MBKzI5Gd+2112bHZs2aVVgfMWJEao8mTZpUWO/Tp09qRFb8AACCEPwAAIIQ/AAAghD8AACCEPwAAIIQ/AAAggh7nEv//v0L60cffXTF13ruueeyY0899VTF1wMqd+qppxbW99tvv4qv9e///u+p3o455pjC+qBBg7I9pVKpsD5nzpxsz0cffbQSs6NRdOvWreKvs+HDh6f2ZuDAgdmxTTfdNEVixQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgiIbe1bvhhhtmx0aNGlW1+1x99dXZsc8//7xq94Ho9thjj+zYkCFDKr7ehx9+WFh/9tlnUy1sueWW2bELL7yw4ustW7assH7aaadle+bPn1/xfWgcvXv3LqyPHTs22zN48ODU3hxxxBHZsbHL+bE2Iit+AABBCH4AAEEIfgAAQQh+AABBCH4AAEEIfgAAQTT0cS4dOuR/eGuttVbF18u96Hzq1Kmp3s4+++yKe1588cXC+u67757tefjhhwvrM2bMqPj+kLPVVlsV1s8777xsT+fOnSu+zzHHHFNYnz17dqqm7373u4X1e++9N9vTtWvXiu8zdOjQwvrjjz9e8bWIIXfMyeTJk1N7tOeeexbWjzzyyGxP9+7dUyRW/AAAghD8AACCEPwAAIIQ/AAAghD8AACCaOhdvdX2zjvvFNabmpqyPaNGjSqsb7TRRtme3//+94X1448/PtuTu16pVMr2NDc3F9ZXW221bM+PfvSjwvr++++f7Zk5c2Z2DIocfvjhFb9oPfe1/p//+Z/Znueffz5VS48ePbJj99xzT2F90003rfg+t956a3bsyiuvrPh6NL7cTtfljQ0cOLCqc1iZ6913330V9/Tu3buwPnbs2GzPzGC/R1nxAwAIQvADAAhC8AMACELwAwAIQvADAAhC8AMACKKptLzzPlbwyJL2diTEV23trtSyZcsq/v+2yirVzdy5o1muueaabM/5559f8XEuuS+X3FE3Zf379y+sz5gxI9XbCn7511R7fNZWRu7rr+ziiy8urK+55prZnokTJxbWDznkkGzPggULUrWObXnssceyPVtuuWXFv3a88MILhfW99947tUeetfo599xzs2O5I4Wuu+66bM+1115bWD/yyCNTNeV+nx48eHC257nnnqv46KaBVT66pq0/a1b8AACCEPwAAIIQ/AAAghD8AACCEPwAAIJo6F29uZ10ZW+88UbV7rO83XyjR48urF9//fXZng022KDiOSxcuLCwvvbaa2d7cjuw7rrrrmxPp06dKp7bW2+9VVj/27/924p/fpa3C3Jl2GlYHauvvnp27LTTTiusn3nmmRU/uzfddFO257bbbius/+EPf0jVNH369MJ6t27dKr7WT37yk+zY8nbkt0eetdaX+xrM7XRdmWstb7ft/fffn+157733Kp7DoEGDCut77rlnxdcaMWLESo21R3b1AgDQQvADAAhC8AMACELwAwAIQvADAAhC8AMACKJDamDHHXdcvaeQxowZU1hvbm7O9hxwwAGF9UmTJmV7fvzjH1c8t9yW/P79+2d7Tj311IrvkzuaY8qUKdmerl27Ftbff//9iu9P69tll12yY8t72XulTjrppOzYdtttV1h/6aWXsj0HH3xwYf3NN9/M9nTv3r3iIxRyxzcNHz482wOV6t27d8VHs8ycObOwftRRR2V77rvvvlQLgwcPrvhomMmTJ4c4suXrsOIHABCE4AcAEITgBwAQhOAHABCE4AcAEERTaQXfnN0eX2a9wQYbZMdee+21wvp6661X8X0ee+yx7NghhxyS2pt11103OzZ+/PjC+qJFi7I9ffr0qXgOtdrV68Xx1XHCCSdkx+68886q3Wd5/29q9XOZm8OLL76Y7TnxxBML66+//nqKwrPW+nK7XWfNmpXtGTRoUGH9+eefT/W25557VrRzd3m7kWu1E7k9PGtW/AAAghD8AACCEPwAAIIQ/AAAghD8AACCEPwAAILokBrYxx9/nB175JFHKjp2YXl233337NgRRxxRWP+P//iPbM/nn3+e6mnevHnZsRtuuKHVj+yg/bn77ruzY7Nnzy6sDxgwINuz/fbbVzyHKVOmVHQkRNmOO+5Y8X3mz59fWL/ooouyPZGObaF1nXvuudmxbt26FdbPO++8bE9bOLYlp3v37oX1sWPHZnsiHduysqz4AQAEIfgBAAQh+AEABCH4AQAEIfgBAATRVFrBN2c32sus+/XrV1gfP358Te5/yimnZMfef//9wvq4ceOyPUuXLi2s9+jRo+L7fO9738v2nHrqqYX1ww47LFVqeS8Oz+22XN6O45XhxfGNo3PnzoX1F154IduzzTbbVHyfM888s7B+yy23VHytSDxrlcnt0H3vvfeyPcOHDy+sDx48OLVHuR26kydPzvaMGDEiRVf6imfNih8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQYY9zWX311QvrP/vZzyo+ymSTTTZJtZB7CX1Z7qdxo402yvZ88sknhfWtt9461cLee++dHZs0aVJN5uCIicZxwQUXFNavvPLKiq91xx13ZMdOO+20iq+HZ61ax7kcccQRDXWUSe7Hubyja7p3757tmTlzZoqu5DgXAADKBD8AgCAEPwCAIAQ/AIAgBD8AgCDC7updGY899lhh/YADDkj1lvv5qdVOulmzZmXHbrzxxsL69ddfn+1pbm5OtWCnYfvy93//99mxoUOHFtYXLVqU7bnmmmsK6yNHjsz2zJ49e7lzpJhnjSIDBw7MjuV2MC+vh2RXLwAA/5/gBwAQhOAHABCE4AcAEITgBwAQhOAHABBEh3pPgPZlzpw5hfUrr7wy23Prrbe24oxoROeee25h/bLLLsv2fP755xUf/fDkk0+uxOyAaskd2VJ2//3313QuUVjxAwAIQvADAAhC8AMACELwAwAIQvADAAiiqbSCb872MuuUOnXqVFhfddVVsz3f/va3C+uHHXZYtme33XYrrO+///7ZnqVLlxbWf/e732V7tthii8L6CSeckO2ZPn16YX3q1KmpPfLi+Prp3Llzduzuu++ueAfgq6++WljffvvtV2J2VJtnLbZu3boV1p977rlsT/fu3VtxRnGfNSt+AABBCH4AAEEIfgAAQQh+AABBCH4AAEEIfgAAQTjOhdAcMVE/w4YNy45ddNFFhfXZs2dne/bdd9/C+ltvvbUSs6PaPGtU+nXh52flOM4FAIAWgh8AQBCCHwBAEIIfAEAQgh8AQBAd6j0BoLHtsccehfULLrig4msNHTo0O2b3LrQ/Y8eOrfcUwrHiBwAQhOAHABCE4AcAEITgBwAQhOAHABCE4AcAEITjXIBWde655xbWO3bsmO0ZPXp0YX3kyJFVmxdQOwMHDiysT548ueZzic6KHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQdvUCX9s666yTHevfv39h/ZNPPsn23HrrrVWZF9C29e7dOzs2YsSIinYIL2+X8MyZM1dido3Jih8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQTaVSqbRC/2FTU+vPBmpsBb/8a8qzRiPyrMXWrVu3wvp7772X7Rk+fHhh/YUXXsj23HfffSm60lc8a1b8AACCEPwAAIIQ/AAAghD8AACCEPwAAIKwq5fQ7DSE2vCsQW3Y1QsAQAvBDwAgCMEPACAIwQ8AIAjBDwAgCMEPACCIFT7OBQCA9s2KHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQgl8bdPnll6empqa03Xbb1Xsq0DBeeumldOaZZ6Ztt902rbnmmql79+5p4MCBadq0afWeGjScBQsWpKFDh6YDDzwwrbfeei2/p9111131nhaCX9sza9asdMUVV7T8xgRUz1VXXZUeeOCB1L9//3T99den0047LT3zzDNp5513Tq+++mq9pwcN5eOPP06XXnppeu2119IOO+xQ7+nwJ5pKpVLpTwvU19FHH50++uijtGzZspYHx29IUB2TJk1Ku+66a+rYseMfa2+++Wbafvvt0xFHHJHuueeeus4PGsnixYvTvHnz0iabbJJefvnltNtuu6WRI0emE088sd5TC8+KXxtSXn24//7703XXXVfvqUDD6dOnz/8JfWU9e/Zs+dZveVUCqJ7VV1+9JfTR9gh+bUR5he+ss85Kp5xySssKBND6yt/w+OCDD9IGG2xQ76kA1ESH2tyGr3LrrbemGTNmpPHjx9d7KhDG6NGj0+zZs1s+iwQQgRW/NmDu3Lnp4osvTkOGDEkbbrhhvacDIbz++uvpjDPOSL17904nnHBCvacDUBOCXxtw0UUXtWx3L3+rF2h977//fjr44IPTN7/5zZbP1a666qr1nhJATfhWb52VdxXefvvtLRs65syZ88f6F198kZqbm9P06dPT2muv3RIMga/v008/TQcddFD65JNP0rPPPpu6dOlS7ykB1IwVvzorf77oyy+/TGeffXbafPPN//jXCy+80HKwbPmfff4IqqP8B6pDDjmk5dn6xS9+kbbZZpt6Twmgpqz41Vn57RwPPvhg4bd/58+f33LQbI8ePeoyN2i0nfNHHXVUmjx5cnr44YdbPtsHEI0DnNuovn37OsAZquicc85p+YNUecWv/Kq2P3fcccfVZV7QqG666aaWj1SUP8Z0yy23pAEDBqSddtqpZaz8mfbyZ2ypPcGvjRL8oPrP1MSJE7PjfimE6tpss81ajikr8u6777aMU3uCHwBAEDZ3AAAEIfgBAAQh+AEABCH4AQAEIfgBAAQh+AEABCH4AQAEscKvbGtqamrdmUAdtMVjLD1rNCLPGrSNZ82KHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBAd6j0B/tL6669fWL/mmmuyPVdddVVh/fXXX6/avACA9s2KHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQdvW2QUOGDCmsH3/88dmed999t7B+6aWXVm1e0Gi6d+9eWO/Zs2eqt5dffrmw/umnn9Z8LvB1HXrooYX1hx9+ONtTKpUK6wcccEC2Z9y4cSsxu1is+AEABCH4AQAEIfgBAAQh+AEABCH4AQAEIfgBAAThOJc2aNttty2sz5kzJ9tzxx13tOKMoG3Ycccds2O77rprYX3AgAHZnh122KGw3qVLl2zPl19+mWrh5ptvLqyfffbZNbk/VNM666xTtedpjz32yI45zuWrWfEDAAhC8AMACELwAwAIQvADAAhC8AMACMKu3jr5l3/5l+xY//79C+tPPfVUtmf27NlVmResjPXXXz87ttZaaxXWhwwZku0ZOHBgYb25uTnbs3jx4sL6O++8k+256aabCuvPPPNMtuc3v/lNqpaRI0dmx37wgx8U1i+77LJsz0cffVSVeUG1nXrqqfWeAv/Nih8AQBCCHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQjnNpZeuuu25hvW/fvhUfSzFmzJiqzQtyvvOd72THfvrTnxbWDzzwwGzPZ599VlifM2dOtuf+++8vrI8YMSLb8/vf/z61N8uWLcuOrbrqqoX1pqamVpwRVP/3u7J11lmnpnMhz4ofAEAQgh8AQBCCHwBAEIIfAEAQgh8AQBB29VbB8nYr3XfffYX1Ll26ZHuefPLJwvodd9yxErODyuy1117ZsaOOOqri633/+98vrD/++OMVXwtou7bddtvs2DbbbFPTuZBnxQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIx7lUwRZbbJEd23fffSu+3hNPPJGqZXkvdD/88MML60uXLs32PPTQQ1WZF23X7bffnh1bsmRJYX2zzTbL9kyaNClFl3sOO3TI/xL84IMPFtY//vjjqs0Lqun000+vyX1eeeWVmtynUVnxAwAIQvADAAhC8AMACELwAwAIQvADAAiiqVQqlb7u7tAounbtWlgfP358tmerrbYqrM+dO7fincCvvvpqqtTyXow9ZcqUwvqYMWOyPd///vdTI1nBL/+a8qw1ntxzmHsGy0455ZTC+siRI1N75FlrHBtuuGFh/Zlnnqn498KV0alTp+xYc3Nziq70Fc+aFT8AgCAEPwCAIAQ/AIAgBD8AgCAEPwCAIAQ/AIAg8m8I5y8cf/zxVdumPmDAgOzYyhzbknPRRRdV7VrAynnggQcK6/Pmzcv2PPfcc604I6j+cS7VPLKlbOLEiYX1L7/8sqr3icaKHwBAEIIfAEAQgh8AQBCCHwBAEIIfAEAQdvX+mV69emXHfvSjH1V8vWuuuaaw/sILL6RaOOSQQyru+e1vf9sqc4FGtt1222XHNt9888L62Wefne2ZNm1aVeYF1XbwwQfX5D6XX355YX3ZsmU1uX+jsuIHABCE4AcAEITgBwAQhOAHABCE4AcAEITgBwAQhONc/szdd9+dHevatWvFL1P/yU9+ktqqJUuWFNbHjRtX87lAe5F7Ef2ECROyPVOmTCms33777VWbF9TKtttuW5P7vPrqqzW5TzRW/AAAghD8AACCEPwAAIIQ/AAAghD8AACCaOhdvR07dqz45c89e/bM9jQ3NxfWzz///Irnttdee2XHOnXqVLUXxK+++urZnpdffrmw/tvf/rbi+0M1de/evbD+3nvv1XXnbtmQIUMK6+uuu27FPx5oq9Zff/3s2IEHHli1+8ycOTM7tnjx4qrdh/9lxQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgCMEPACCIhj7OZXnHKwwaNKhq9xk7dmx2bN68eRUfF5E7hqZUKqVq2mijjSqql3344YdVnQPtS9euXQvrBx10ULand+/ehfXddtut4qMk5s6dm2qhS5cuFf+6MnXq1GzP0qVLqzIvqJXVVlstO7bhhhtW7T6//OUvs2OffPJJ1e7D/7LiBwAQhOAHABCE4AcAEITgBwAQhOAHABBEU2kFt4o2NTWl9mbjjTfOjs2ZMye1VausUpzHv/zyy5rc//bbb8+O/fCHP0yNpNo7pauh3s/aKaeckh274YYbKv7/+MQTTxTWX3755WzPv/3bvxXW33///VSpb3/729mx8ePHF9ZXXXXVbM/ChQsrfql97mXz55xzTrZn1KhRhfXm5ubUHnnW2pdNNtkkOzZ79uyq3Wf33XfPjr3yyitVu08kpa941qz4AQAEIfgBAAQh+AEABCH4AQAEIfgBAAQh+AEABNEhNbBPP/00OzZ48ODC+qWXXprtWWONNQrrL730UqrFEQOPPPJItqdz586F9Z/+9KfZnkWLFhXWb7rppq+cI41recf5vPrqqxUfAfPiiy+mWthiiy0qPjJl7bXXLqwfe+yxFR81c9BBB2V7hgwZUvH/6x122KGwfvXVV2d7Zs6cmR2DSuy9995Vvd4bb7xRWJ8xY0ZV78NXs+IHABCE4AcAEITgBwAQhOAHABCE4AcAEERTaQXfnB3lZdbbbrttxbt6l/ey+XqbP39+dmzOnDmF9T59+mR75s6dmxqJF8f/pddff73iXaP7779/qoUrrrgiO/Z3f/d3hfUlS5Zke04++eTC+rhx41I1rbbaaoX1Cy+8MNtzxhlnVHxaQc+ePVNb5VlrX4YPH54d+/GPf1zx9R599NHC+t/8zd9UfC2+3rNmxQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIx7k0uOUd59KpU6fCeu/evbM9bfnompXhiIm/1LVr14qPZJg3b162Z/z48YX1v/qrv8r2DBw4sLC+9tprZ3tGjx5dWD/vvPOyPcs7GqXe9txzz8L6Qw89lO1pbm4urN9www3ZnltuuaWwvmDBglRNnrW2qUOHDoX1KVOmZHu22mqriu/jOJfacZwLAAAtBD8AgCAEPwCAIAQ/AIAgBD8AgCDs6m1w06dPz47NmTOnsN6nT58UhZ2GlenSpUvFO2f79+9f8X3eeOONwvrPf/7zMDvOV8Z3v/vdwnqvXr0qvtYdd9yRqsmz1jYdd9xxhfW77767qvf5wx/+UFjv27dvtmfu3LlVnUMUJbt6AQAoE/wAAIIQ/AAAghD8AACCEPwAAIIQ/AAAgih+OzMN4/DDD8+O3XjjjTWdC+1f7gigQYMG1Xwu/KVf/epXFdVhyZIlNbnP22+/XVh3ZEvtWfEDAAhC8AMACELwAwAIQvADAAhC8AMACMKu3gY3bdq0lRoDoPGNHTu2sH7AAQdke0488cSK7/O73/2u4h5ahxU/AIAgBD8AgCAEPwCAIAQ/AIAgBD8AgCAEPwCAIJpKpVJphf7DpqbWnw3U2Ap++deUZ41G5FlrX4466qjs2L333ltYv/7667M9F1xwQWF96dKlKzE7vs6zZsUPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIAi7egnNTkOoDc8a1IZdvQAAtBD8AACCEPwAAIIQ/AAAghD8AACCEPwAAIIQ/AAAghD8AACCEPwAAIIQ/AAAghD8AACCEPwAAIIQ/AAAghD8AACCEPwAAIIQ/AAAghD8AACCEPwAAIIQ/AAAgmgqlUqlek8CAIDWZ8UPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgCMEPACAIwQ8AIAjBDwAgxfD/ADtbyU6NiDh/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_dataset), size=(1,)).item()\n",
    "    img, label = training_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape X : torch.Size([64, 1, 28, 28])\n",
      "shape y : torch.Size([64])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_dataset, batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f'shape X : {X.shape}')\n",
    "    print(f'shape y : {y.shape}')\n",
    "    print(type(y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fcs = nn.ModuleList(\n",
    "            [nn.Linear(dims[i], dims[i+1]) for i in range(len(dims) - 1)]\n",
    "        )\n",
    "        self.act = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x): # fait : 1 * 28 * 28 --> 1 * 784\n",
    "        x = self.flatten(x)\n",
    "        for layer in self.fcs:\n",
    "            x = layer(x)\n",
    "            x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fcs): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       "  (act): ReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "\n",
    "model = MLP([input_dim, 64, 64, output_dim])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # Expect raw logits\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # Prédiction\n",
    "        ypred = model(X)\n",
    "        loss = loss_fn(ypred, y)\n",
    "\n",
    "        # Backpropag\n",
    "        loss.backward()\n",
    "        optimizer.step() # W = W - lr * grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size * len(X)\n",
    "            # print(f'loss {loss:>7f} [{current:>5d}/{len(dataloader.dataset)}]')\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            ypred = model(X)\n",
    "            test_loss += loss_fn(ypred, y).item()\n",
    "            correct += (ypred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss/=len(dataloader)\n",
    "    correct/= len(dataloader.dataset)\n",
    "    print(f'Test loss : {test_loss:>8f} | test accuracy {(correct * 100):7f}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    continue\n",
    "    print(f'Epoch {t+1} ----------------------------------')\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jour 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.13092496991157532\n",
      "Variance: 0.30183959007263184\n"
     ]
    }
   ],
   "source": [
    "# Mean, Variance des images\n",
    "\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "mean, std = 0, 0\n",
    "combined_data = ConcatDataset([training_dataset, test_dataset])\n",
    "\n",
    "for image, label in combined_data:\n",
    "    image = image.view(28*28)\n",
    "    mean+= image.mean().sum()\n",
    "    std+= image.std().sum()\n",
    "\n",
    "mean /= len(combined_data)\n",
    "std /= len(combined_data)\n",
    "\n",
    "print (f'Mean: {mean}')\n",
    "print (f'Variance: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 1.2540766647362034e-06\n",
      "Variance: 0.9999989867210388\n"
     ]
    }
   ],
   "source": [
    "training_dataset2 = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=T.Compose([T.ToTensor(), T.Normalize((mean), (std))])\n",
    ")\n",
    "\n",
    "test_dataset2 = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=T.Compose([T.ToTensor(), T.Normalize((mean), (std))])\n",
    ")\n",
    "\n",
    "# Mean, Variance des images\n",
    "\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "mean, std = 0, 0\n",
    "combined_data = ConcatDataset([training_dataset2, test_dataset2])\n",
    "\n",
    "for image, label in combined_data:\n",
    "    image = image.view(28*28)\n",
    "    mean+= image.mean().sum()\n",
    "    std+= image.std().sum()\n",
    "\n",
    "mean /= len(combined_data)\n",
    "std /= len(combined_data)\n",
    "\n",
    "print (f'Mean: {mean}')\n",
    "print (f'Variance: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.3948, -0.1999,\n",
       "           -0.1999, -0.1999,  1.2033,  1.3332,  1.8399, -0.0960,  1.7230,\n",
       "            2.8793,  2.7753,  1.2163, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.0440,  0.0340,  0.7875,  1.5670,  1.7749,  2.8533,\n",
       "            2.8533,  2.8533,  2.8533,  2.8533,  2.4895,  1.8009,  2.8533,\n",
       "            2.7104,  2.0997,  0.3977, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "            0.2029,  2.6584,  2.8533,  2.8533,  2.8533,  2.8533,  2.8533,\n",
       "            2.8533,  2.8533,  2.8533,  2.8273,  0.7745,  0.6316,  0.6316,\n",
       "            0.2938,  0.0729, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.1999,  2.4115,  2.8533,  2.8533,  2.8533,  2.8533,  2.8533,\n",
       "            2.1387,  1.9308,  2.7753,  2.6974, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338,  0.6056,  1.5930,  0.9564,  2.8533,  2.8533,  2.2297,\n",
       "           -0.2908, -0.4338,  0.1249,  1.5670, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.2519, -0.4208,  1.5670,  2.8533,  0.7355,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338,  1.3722,  2.8533,  2.0348,\n",
       "           -0.4078, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.2908,  2.0348,  2.8533,\n",
       "            0.4757, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,  0.0210,  2.6974,\n",
       "            2.4895,  1.6450,  0.9694, -0.4208, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,  0.6186,\n",
       "            2.6844,  2.8533,  2.8533,  1.1123, -0.1090, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "            0.1509,  1.9828,  2.8533,  2.8533,  1.5151, -0.0830, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.2259,  0.7745,  2.8403,  2.8533,  1.9958, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338,  2.8013,  2.8533,  2.8013,  0.3977,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "            0.1639,  1.2552,  1.9438,  2.8533,  2.8533,  2.2556, -0.4078,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,  0.0729,  1.4891,\n",
       "            2.5415,  2.8533,  2.8533,  2.8533,  2.8143,  1.9308, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.1219,  1.0474,  2.4375,  2.8533,\n",
       "            2.8533,  2.8533,  2.8533,  2.1777,  0.5796, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.1349,  0.4237,  2.3336,  2.8533,  2.8533,  2.8533,\n",
       "            2.8533,  2.1387,  0.6186, -0.4078, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.1999,\n",
       "            1.7879,  2.4115,  2.8533,  2.8533,  2.8533,  2.8533,  2.0997,\n",
       "            0.6056, -0.3168, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338,  0.2808,  1.8009,  2.5025,\n",
       "            2.8533,  2.8533,  2.8533,  2.8533,  2.7363,  1.2942, -0.2908,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338,  1.3332,  2.8533,  2.8533,\n",
       "            2.8533,  2.3206,  1.3202,  1.2812, -0.2259, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338],\n",
       "          [-0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338,\n",
       "           -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338, -0.4338]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape X : torch.Size([64, 1, 28, 28])\n",
      "shape y : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader2 = DataLoader(training_dataset2, batch_size, shuffle=True)\n",
    "test_dataloader2 = DataLoader(test_dataset2, batch_size, shuffle=True)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f'shape X : {X.shape}')\n",
    "    print(f'shape y : {y.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done !\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    continue\n",
    "    print(f'Epoch {t+1} ----------------------------------')\n",
    "    train_loop(train_dataloader2, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader2, model, loss_fn)\n",
    "\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On améliore tout ça\n",
    "\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "input_dim = 28*28\n",
    "output_dim = 10 #Nombre de classe\n",
    "model = MLP([input_dim, 128, 128, output_dim]).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/mnist_experiment_1')\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    return epoch_loss / len(dataloader), correct / len(dataloader.dataset) * 100\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    return test_loss / len(dataloader), correct / len(dataloader.dataset) * 100\n",
    "\n",
    "# Training loop with added graphing\n",
    "def train_and_test(dataloader_train, dataloader_test, model, loss_fn, optimizer, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Train the model\n",
    "        train_loss, accuracy = train_loop(dataloader_train, model, loss_fn, optimizer, batch_size)\n",
    "        writer.add_scalars('Loss', {'train': train_loss}, epoch)\n",
    "        writer.add_scalars('Accuracy', {'train': accuracy}, epoch)\n",
    "\n",
    "        # Test the model\n",
    "        test_loss, accuracy = test_loop(dataloader_test, model, loss_fn)\n",
    "        writer.add_scalars('Loss', {'test': test_loss}, epoch)\n",
    "        writer.add_scalars('Accuracy', {'test': accuracy}, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} complete\\n\")\n",
    "\n",
    "#train_and_test(train_dataloader2, test_dataloader2, model, loss_fn, optimizer, epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=runs/mnist_experiment_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circonvolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv_stack_activation = nn.ReLU\n",
    "        self.mlp_activation = nn.ReLU\n",
    "\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            self.conv_stack_activation(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            self.conv_stack_activation(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # 64 * 28 * 28 -> 64 * 14 * 14\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 14 * 14, 128),\n",
    "            self.mlp_activation(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            self.mlp_activation(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(cnn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 1/10 complete\n",
      "\n",
      "Epoch 2/10\n",
      "Epoch 2/10 complete\n",
      "\n",
      "Epoch 3/10\n",
      "Epoch 3/10 complete\n",
      "\n",
      "Epoch 4/10\n",
      "Epoch 4/10 complete\n",
      "\n",
      "Epoch 5/10\n",
      "Epoch 5/10 complete\n",
      "\n",
      "Epoch 6/10\n",
      "Epoch 6/10 complete\n",
      "\n",
      "Epoch 7/10\n",
      "Epoch 7/10 complete\n",
      "\n",
      "Epoch 8/10\n",
      "Epoch 8/10 complete\n",
      "\n",
      "Epoch 9/10\n",
      "Epoch 9/10 complete\n",
      "\n",
      "Epoch 10/10\n",
      "Epoch 10/10 complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('runs/mnist_experiment_2')\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "train_and_test(train_dataloader2, test_dataloader2, cnn, loss_fn, optimizer, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "I0128 11:40:22.353121 6123122688 plugin.py:429] Monitor runs begin\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.18.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# !tensorboard --logdir=runs/mnist_experiment_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input : Float(*, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cpu),\n",
      "      %conv_stack.0.weight : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv_stack.0.bias : Float(32, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_stack.3.weight : Float(64, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv_stack.3.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_stack.6.weight : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_stack.6.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv_stack.6.running_mean : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %conv_stack.6.running_var : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %mlp.1.weight : Float(128, 12544, strides=[12544, 1], requires_grad=1, device=cpu),\n",
      "      %mlp.1.bias : Float(128, strides=[1], requires_grad=1, device=cpu),\n",
      "      %mlp.4.weight : Float(128, 128, strides=[128, 1], requires_grad=1, device=cpu),\n",
      "      %mlp.4.bias : Float(128, strides=[1], requires_grad=1, device=cpu),\n",
      "      %mlp.7.weight : Float(10, 128, strides=[128, 1], requires_grad=1, device=cpu),\n",
      "      %mlp.7.bias : Float(10, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %/conv_stack/conv_stack.0/Conv_output_0 : Float(*, 32, 28, 28, strides=[25088, 784, 28, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/conv_stack/conv_stack.0/Conv\"](%input, %conv_stack.0.weight, %conv_stack.0.bias), scope: __main__.CNN::/torch.nn.modules.container.Sequential::conv_stack/torch.nn.modules.conv.Conv2d::conv_stack.0 # /Users/felix/dev_iim/python_and_ai/Deep Learning/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/conv_stack/conv_stack.1/Relu_output_0 : Float(*, 32, 28, 28, strides=[25088, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/conv_stack/conv_stack.1/Relu\"](%/conv_stack/conv_stack.0/Conv_output_0), scope: __main__.CNN::/torch.nn.modules.container.Sequential::conv_stack/torch.nn.modules.activation.ReLU::conv_stack.1 # /Users/felix/dev_iim/python_and_ai/Deep Learning/.venv/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/conv_stack/conv_stack.3/Conv_output_0 : Float(*, 64, 28, 28, strides=[50176, 784, 28, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/conv_stack/conv_stack.3/Conv\"](%/conv_stack/conv_stack.1/Relu_output_0, %conv_stack.3.weight, %conv_stack.3.bias), scope: __main__.CNN::/torch.nn.modules.container.Sequential::conv_stack/torch.nn.modules.conv.Conv2d::conv_stack.3 # /Users/felix/dev_iim/python_and_ai/Deep Learning/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:549:0\n",
      "  %/conv_stack/conv_stack.4/Relu_output_0 : Float(*, 64, 28, 28, strides=[50176, 784, 28, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/conv_stack/conv_stack.4/Relu\"](%/conv_stack/conv_stack.3/Conv_output_0), scope: __main__.CNN::/torch.nn.modules.container.Sequential::conv_stack/torch.nn.modules.activation.ReLU::conv_stack.4 # /Users/felix/dev_iim/python_and_ai/Deep Learning/.venv/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/conv_stack/conv_stack.6/BatchNormalization_output_0 : Float(*, 64, 28, 28, strides=[50176, 784, 28, 1], requires_grad=1, device=cpu) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002, onnx_name=\"/conv_stack/conv_stack.6/BatchNormalization\"](%/conv_stack/conv_stack.4/Relu_output_0, %conv_stack.6.weight, %conv_stack.6.bias, %conv_stack.6.running_mean, %conv_stack.6.running_var), scope: __main__.CNN::/torch.nn.modules.container.Sequential::conv_stack/torch.nn.modules.batchnorm.BatchNorm2d::conv_stack.6 # /Users/felix/dev_iim/python_and_ai/Deep Learning/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2812:0\n",
      "  %/conv_stack/conv_stack.7/MaxPool_output_0 : Float(*, 64, 14, 14, strides=[12544, 196, 14, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1, 1], kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2], onnx_name=\"/conv_stack/conv_stack.7/MaxPool\"](%/conv_stack/conv_stack.6/BatchNormalization_output_0), scope: __main__.CNN::/torch.nn.modules.container.Sequential::conv_stack/torch.nn.modules.pooling.MaxPool2d::conv_stack.7 # /Users/felix/dev_iim/python_and_ai/Deep Learning/.venv/lib/python3.11/site-packages/torch/nn/functional.py:830:0\n",
      "  %/mlp/mlp.0/Flatten_output_0 : Float(*, 12544, strides=[12544, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1, onnx_name=\"/mlp/mlp.0/Flatten\"](%/conv_stack/conv_stack.7/MaxPool_output_0), scope: __main__.CNN::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.flatten.Flatten::mlp.0 # /Users/felix/dev_iim/python_and_ai/Deep Learning/.venv/lib/python3.11/site-packages/torch/nn/modules/flatten.py:53:0\n",
      "  %/mlp/mlp.1/Gemm_output_0 : Float(*, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/mlp/mlp.1/Gemm\"](%/mlp/mlp.0/Flatten_output_0, %mlp.1.weight, %mlp.1.bias), scope: __main__.CNN::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.linear.Linear::mlp.1 # /Users/felix/dev_iim/python_and_ai/Deep Learning/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/mlp/mlp.2/Relu_output_0 : Float(*, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/mlp/mlp.2/Relu\"](%/mlp/mlp.1/Gemm_output_0), scope: __main__.CNN::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.activation.ReLU::mlp.2 # /Users/felix/dev_iim/python_and_ai/Deep Learning/.venv/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %/mlp/mlp.4/Gemm_output_0 : Float(*, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/mlp/mlp.4/Gemm\"](%/mlp/mlp.2/Relu_output_0, %mlp.4.weight, %mlp.4.bias), scope: __main__.CNN::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.linear.Linear::mlp.4 # /Users/felix/dev_iim/python_and_ai/Deep Learning/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  %/mlp/mlp.5/Relu_output_0 : Float(*, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/mlp/mlp.5/Relu\"](%/mlp/mlp.4/Gemm_output_0), scope: __main__.CNN::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.activation.ReLU::mlp.5 # /Users/felix/dev_iim/python_and_ai/Deep Learning/.venv/lib/python3.11/site-packages/torch/nn/functional.py:1704:0\n",
      "  %output : Float(*, 10, strides=[10, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/mlp/mlp.7/Gemm\"](%/mlp/mlp.5/Relu_output_0, %mlp.7.weight, %mlp.7.bias), scope: __main__.CNN::/torch.nn.modules.container.Sequential::mlp/torch.nn.modules.linear.Linear::mlp.7 # /Users/felix/dev_iim/python_and_ai/Deep Learning/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformation pour avoir un modele exportable\n",
    "\n",
    "cnn.to(\"cpu\")\n",
    "\n",
    "torch_input = torch.randn(1, 1, 28, 28) # N, C, L, T\n",
    "\n",
    "onn_program = torch.onnx.export(\n",
    "    cnn,\n",
    "    torch_input,\n",
    "    \"model.onnx\",\n",
    "    verbose=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    opset_version=11, # /!\\ \n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batchs_size\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 1\n",
      "Sample input: [tensor([[[[-1.2219e+00, -2.5367e-01,  6.8956e-01,  6.9917e-01, -1.2046e+00,\n",
      "           -2.9086e-01,  1.4856e+00,  3.3892e-01, -4.9007e-01, -7.7641e-01,\n",
      "            5.5521e-01, -2.2366e-01, -6.7772e-01, -3.5954e-01, -7.7353e-01,\n",
      "           -5.5444e-01,  7.9168e-01,  4.9112e-01, -1.2879e-01, -6.2677e-01,\n",
      "           -9.9835e-01, -1.2090e+00, -9.3111e-01, -2.7487e-01,  1.7910e+00,\n",
      "            1.8780e-01,  8.0235e-01,  2.4094e+00],\n",
      "          [-5.5647e-03,  1.1229e+00, -9.0257e-01, -9.1104e-02, -8.9972e-02,\n",
      "            2.3331e-01, -7.0786e-01,  2.8732e+00,  9.0988e-01,  1.5263e+00,\n",
      "           -1.8106e+00, -2.1439e-01,  1.2391e+00, -1.5240e+00, -1.1004e+00,\n",
      "            1.1597e+00, -1.1166e+00,  1.3053e+00, -1.7253e+00,  1.2159e+00,\n",
      "           -2.9961e-01,  1.5998e+00, -7.9291e-01,  3.4495e-01,  1.3541e+00,\n",
      "            2.0128e-01,  8.1394e-01, -2.2371e-01],\n",
      "          [ 1.6610e-01, -8.1934e-01, -1.0818e+00,  1.5761e+00, -9.1697e-01,\n",
      "           -8.5459e-01,  4.9244e-01, -3.7366e-01,  5.0302e-03, -9.1266e-01,\n",
      "            9.4684e-01,  1.1295e+00,  1.0949e+00,  2.8880e-01, -7.0919e-01,\n",
      "           -1.2102e+00,  9.1916e-02,  1.3049e+00, -9.4696e-01,  3.8923e-01,\n",
      "           -1.3601e-01,  5.8906e-01,  7.9454e-01,  8.8428e-01, -9.3958e-01,\n",
      "           -1.1904e+00, -4.6127e-01,  1.0462e+00],\n",
      "          [ 1.9238e-01, -9.1283e-01,  1.2764e+00, -9.4363e-01,  5.9919e-02,\n",
      "           -4.2566e-01,  1.0140e+00,  2.4971e-01, -8.2393e-01, -1.8682e-01,\n",
      "           -8.3233e-01,  4.3247e-01,  3.8196e-01,  2.2452e+00,  3.4753e-01,\n",
      "           -1.8342e+00,  4.0595e-01,  2.0872e-01,  5.2964e-01,  7.6603e-01,\n",
      "           -1.3815e+00,  6.2640e-01,  1.5263e+00,  4.1449e-01,  1.3065e-01,\n",
      "            8.5190e-01, -1.2408e+00, -6.3312e-01],\n",
      "          [ 2.0110e-01,  6.9197e-01,  2.3326e-01,  2.6633e-01,  3.6448e-01,\n",
      "           -2.8358e-01, -5.7763e-02, -7.3833e-01,  1.5029e+00, -2.3337e-01,\n",
      "            8.2558e-01, -8.3949e-01, -1.7350e+00, -4.8710e-01,  3.5458e-01,\n",
      "            6.9139e-01, -4.8267e-01, -6.4256e-01,  4.4826e-01,  2.4589e+00,\n",
      "            1.8142e-01,  5.9192e-02, -8.5580e-02, -8.8829e-01,  5.9508e-01,\n",
      "           -1.4195e+00, -4.3169e-02,  2.8146e-01],\n",
      "          [-1.2742e+00,  7.3543e-02, -5.4816e-01, -4.9419e-03, -7.4062e-01,\n",
      "           -7.6881e-01,  9.0417e-01,  2.7559e-02, -1.0355e+00, -1.9158e+00,\n",
      "           -3.0166e-01, -2.9811e-01,  2.0968e-01, -5.1288e-01,  1.1737e+00,\n",
      "           -1.2192e+00, -6.1904e-01,  1.8813e+00,  1.3151e-01, -1.2699e+00,\n",
      "           -9.4227e-01, -3.7538e-01,  7.6605e-01,  4.0165e-01,  1.4404e+00,\n",
      "            9.1694e-02,  6.1403e-01, -1.6833e+00],\n",
      "          [-7.5237e-01, -3.0351e-01,  4.6841e-01, -3.0962e-01,  8.0429e-02,\n",
      "            8.4937e-01, -1.4611e+00,  3.2878e-01, -3.7261e-01,  2.6698e-01,\n",
      "            2.0588e-01, -3.6237e-01, -1.6095e-01,  1.5394e+00, -8.5147e-01,\n",
      "           -6.7572e-02,  7.2528e-01, -2.6753e-01, -9.7835e-01,  8.2179e-01,\n",
      "           -3.8036e-01, -2.4919e-01,  1.1330e+00,  1.7994e+00, -4.5716e-02,\n",
      "           -1.7922e+00,  9.3446e-01,  1.8805e-01],\n",
      "          [-2.8555e-01,  4.5461e-01,  9.6895e-01, -4.8215e-01, -7.4269e-02,\n",
      "           -1.8695e-01,  2.5374e-01, -1.6092e+00,  6.5561e-01, -1.0030e+00,\n",
      "            4.6924e-03,  1.1327e+00, -8.1872e-01, -2.7002e+00,  1.0980e+00,\n",
      "           -1.5297e+00,  8.9324e-03, -3.1335e-01,  3.6068e-01,  6.1226e-01,\n",
      "            1.3606e-01,  9.5309e-01, -1.7100e-01,  9.5113e-01, -4.8006e-01,\n",
      "           -1.4827e+00,  6.9575e-01,  3.0289e-01],\n",
      "          [ 3.4463e-01,  1.3119e+00, -5.9991e-01,  9.8043e-01,  3.4001e-02,\n",
      "           -4.5449e-01, -1.1841e+00, -7.7326e-01,  2.8665e-01,  1.8876e-01,\n",
      "           -3.5307e-01,  1.1234e-01, -1.1496e-01,  4.0034e-01, -1.0302e+00,\n",
      "           -1.4740e+00,  4.2304e-01,  4.7683e-01, -1.4246e-01,  1.5912e+00,\n",
      "            1.9932e-02,  3.6749e+00, -2.2868e-02, -8.9620e-01, -1.0981e+00,\n",
      "            3.3151e-01, -6.4815e-01,  1.5171e+00],\n",
      "          [ 7.5631e-01,  1.2261e+00, -7.4488e-01, -3.4137e-01, -4.2735e-01,\n",
      "           -1.2880e+00, -9.5872e-01,  8.6650e-01, -1.3590e+00,  3.6101e-01,\n",
      "            1.8314e+00,  1.1438e+00,  1.7475e-01, -3.9947e-01,  7.6214e-01,\n",
      "           -1.8792e-01, -4.7584e-01,  3.0126e-01,  6.8354e-02,  1.4165e+00,\n",
      "           -3.7197e-01,  1.8256e-02,  1.3316e+00, -1.5517e+00, -1.3638e-01,\n",
      "            9.8134e-01, -1.8279e+00, -1.0711e+00],\n",
      "          [-1.0104e+00, -1.2073e+00,  3.0017e-01, -7.3405e-01, -8.8352e-01,\n",
      "            4.4815e-01,  1.9993e+00,  7.2476e-01,  4.6068e-01, -8.0176e-01,\n",
      "           -1.9527e-01, -8.1525e-01, -4.6056e-01, -5.6469e-01,  8.5272e-01,\n",
      "           -6.9099e-01,  1.7653e-01,  1.4195e+00, -6.0978e-01,  1.7417e+00,\n",
      "            2.8853e-02,  3.5087e-01, -7.4423e-01,  2.8059e-01, -4.6264e-01,\n",
      "           -1.1045e+00, -4.6433e-01, -1.7591e-01],\n",
      "          [-6.9620e-01,  8.3254e-01,  2.1419e-02,  2.5042e+00, -1.7503e+00,\n",
      "           -7.2429e-01, -1.5843e+00,  1.3233e+00, -4.0665e-01, -2.3885e+00,\n",
      "            4.7228e-01, -1.5321e+00, -1.7099e+00,  6.9573e-01, -1.3532e+00,\n",
      "           -1.7664e-01, -1.4819e+00, -9.7089e-01,  1.1503e+00,  2.0751e-01,\n",
      "           -1.2100e+00,  1.1807e+00,  2.4585e+00,  3.5634e-01, -3.1773e-01,\n",
      "           -9.0010e-01,  1.2043e+00, -4.8650e-01],\n",
      "          [-1.9242e+00,  1.1492e+00, -7.7547e-01, -3.7084e-01, -6.6005e-01,\n",
      "            6.1947e-01,  1.3197e+00,  2.9459e-01,  1.0237e+00,  9.3139e-01,\n",
      "           -5.9762e-01, -5.7481e-01,  1.9094e-01,  3.9171e-03, -8.2663e-01,\n",
      "            1.1078e+00,  8.4003e-01, -3.1670e-01,  8.3061e-01, -7.1307e-01,\n",
      "            2.7500e-01,  1.9895e-01,  6.9382e-01, -9.8144e-01, -1.9373e+00,\n",
      "            5.9246e-01,  8.2419e-02,  4.3292e-01],\n",
      "          [ 1.2632e+00,  1.2576e+00,  2.7452e-01, -9.9873e-01,  9.0627e-01,\n",
      "           -1.7701e+00,  2.7021e-01, -1.4526e+00, -6.2769e-01, -7.9888e-01,\n",
      "            9.5024e-02,  1.4736e+00,  5.1975e-01,  2.4402e+00,  4.2911e-03,\n",
      "           -7.7300e-01, -7.3712e-01,  1.5787e+00,  1.4574e+00, -5.8176e-02,\n",
      "           -2.4452e-02,  3.0268e-01,  6.5006e-01, -2.0339e+00,  2.8365e-01,\n",
      "            5.6275e-01,  7.9309e-01, -1.5829e-01],\n",
      "          [ 9.6585e-01, -7.3471e-01, -3.1840e-01, -2.2240e+00, -1.3401e+00,\n",
      "           -1.5568e+00,  7.7729e-02,  2.7262e-01,  9.3366e-02,  4.9422e-01,\n",
      "           -8.8907e-01,  3.8993e-01, -1.4198e-01,  3.7773e-01,  1.5736e+00,\n",
      "           -4.0380e-02, -4.9042e-01, -7.1710e-01,  2.5197e-01, -2.0479e+00,\n",
      "           -1.7786e+00, -4.9881e-01, -4.7635e-01,  3.8104e-01,  1.6560e-01,\n",
      "            1.3007e+00, -2.0381e+00, -2.0176e-01],\n",
      "          [ 2.7812e-01,  3.6144e-01, -3.7139e-02,  1.6099e-01,  6.9413e-01,\n",
      "            1.7763e+00,  1.7826e+00,  4.3620e-01,  3.7612e-01, -2.2289e-01,\n",
      "            1.2191e+00,  8.2127e-01,  1.1020e+00,  2.1262e-01, -1.9689e-01,\n",
      "           -4.4415e-01,  1.3041e+00,  1.1430e+00, -7.4864e-01,  3.7953e-01,\n",
      "           -6.5946e-01, -7.9047e-01,  4.5649e-01, -4.4470e-01, -9.8546e-01,\n",
      "            1.2535e+00, -4.4217e-01, -1.6920e+00],\n",
      "          [-2.0874e-01, -3.0823e-01,  2.0966e+00, -3.1536e-01, -6.8960e-01,\n",
      "           -1.9168e-01, -2.8777e-01, -4.7175e-01, -1.1523e+00, -1.2647e+00,\n",
      "            6.4389e-01, -7.8880e-01,  5.5809e-01,  1.8933e-01, -1.2558e+00,\n",
      "           -3.5320e-01, -4.4962e-01, -6.2398e-02, -8.4915e-01, -1.6222e+00,\n",
      "           -8.8171e-01, -9.8165e-02,  2.4723e-01, -4.9200e-01,  3.3146e-01,\n",
      "            5.2725e-01,  6.1848e-01, -7.6121e-01],\n",
      "          [ 9.3812e-01,  1.1325e+00, -4.4683e-01,  2.0997e+00, -1.4596e-01,\n",
      "           -9.5467e-02, -1.1000e+00,  1.9667e+00,  8.7704e-01,  9.1670e-02,\n",
      "           -1.6515e+00,  9.5668e-01, -5.4821e-01, -2.0638e+00,  3.0096e-01,\n",
      "           -4.2055e-01, -9.3453e-01,  5.4835e-01,  5.4984e-01,  2.5059e-01,\n",
      "            1.4614e+00, -2.0762e+00,  6.7980e-01, -1.4171e+00,  1.4985e+00,\n",
      "            1.1810e+00, -7.4561e-01,  6.0321e-02],\n",
      "          [ 2.2427e+00,  1.8391e+00, -1.2912e-01,  1.4555e+00,  4.4200e-01,\n",
      "           -5.8880e-01,  1.1637e-01, -2.4915e-01,  2.2280e-01, -2.0150e-01,\n",
      "           -1.0723e+00,  7.4254e-01,  2.1861e+00,  2.4802e+00, -1.5848e+00,\n",
      "            1.5070e+00,  5.8716e-01, -5.2806e-01,  5.9541e-01,  9.5385e-01,\n",
      "           -4.9868e-02, -1.0436e+00,  1.3300e+00,  4.2849e-01,  1.1754e+00,\n",
      "            1.6401e+00, -5.0560e-01, -8.3529e-01],\n",
      "          [ 8.0216e-02, -7.0707e-01, -1.6057e+00, -2.1349e+00,  1.0934e+00,\n",
      "           -2.0119e-01,  8.2389e-01, -1.8803e+00,  1.9635e-01, -3.4621e+00,\n",
      "            1.7674e+00,  1.4382e+00, -1.0383e+00,  1.2923e+00,  2.9713e-01,\n",
      "            1.7943e-01, -1.0938e+00,  5.7447e-01,  1.5595e+00,  6.8438e-01,\n",
      "            3.1554e-01,  5.3647e-01, -9.4031e-02, -3.6654e-01, -9.6750e-01,\n",
      "            5.6305e-01, -9.0516e-01,  3.4395e-01],\n",
      "          [-4.0338e-01, -1.2574e+00, -2.0966e-02,  9.9135e-01,  7.9210e-02,\n",
      "            1.5776e+00, -5.1775e-01, -4.7009e-01,  1.0326e+00, -9.3882e-01,\n",
      "           -1.1599e+00,  2.3191e+00,  6.3447e-01, -6.9676e-01, -6.4398e-01,\n",
      "            1.3202e+00, -8.3811e-01, -1.7534e+00,  2.5750e-01, -1.0123e+00,\n",
      "            4.6585e-01,  2.0751e+00,  4.7752e-01,  2.5128e-01, -2.9233e+00,\n",
      "            1.8826e+00,  2.8983e-01, -1.3249e-01],\n",
      "          [-9.9841e-02,  7.3009e-01, -1.4673e-01,  1.0334e+00,  6.3944e-01,\n",
      "           -1.6027e-01, -4.4598e-01,  1.3582e+00,  1.3564e+00,  3.2979e-01,\n",
      "           -1.1397e+00,  2.6413e-01, -4.3906e-01, -9.8503e-01,  1.9912e-01,\n",
      "           -7.8679e-01,  1.7881e-04,  5.1145e-01, -3.6292e-01, -1.2661e+00,\n",
      "           -7.3219e-01,  2.1936e+00,  6.4127e-01,  4.7432e-01,  6.6892e-01,\n",
      "           -1.0387e+00,  2.7661e-01,  1.1183e+00],\n",
      "          [ 1.0116e+00, -6.9395e-02, -1.9036e+00, -5.7112e-01,  1.0467e+00,\n",
      "            8.5699e-01, -6.2954e-02, -1.9147e+00, -8.0294e-01, -2.2807e-01,\n",
      "            2.9220e-01, -6.7354e-01, -5.6299e-01, -2.1235e-01, -4.9112e-01,\n",
      "            6.3609e-01,  1.6368e-01,  1.0199e+00, -1.2440e+00, -1.2312e+00,\n",
      "            9.7823e-01, -6.4570e-01,  4.3400e-02, -2.0156e-01, -2.4588e-02,\n",
      "           -3.2358e-01, -1.2159e+00,  8.6597e-01],\n",
      "          [-3.6429e-01, -3.5225e-01,  5.1068e-01,  1.2517e+00,  9.3336e-01,\n",
      "            3.5456e-01, -6.2317e-01,  2.5174e-01,  4.6418e-01, -1.3732e-01,\n",
      "            2.1486e+00,  1.5249e+00, -1.4095e+00,  7.6345e-01,  1.0511e+00,\n",
      "           -3.3734e-01,  1.3220e+00, -4.4437e-01, -6.9795e-01, -4.6839e-02,\n",
      "            1.6133e+00, -8.6369e-01,  1.3361e+00, -5.4562e-01, -5.1232e-01,\n",
      "            1.1571e+00,  9.1881e-02, -9.6423e-01],\n",
      "          [ 2.7631e-01,  7.3603e-01,  5.3477e-01,  4.4176e-01,  9.1563e-01,\n",
      "           -2.4898e-01,  6.3618e-01,  8.4167e-01, -5.6045e-01, -2.3181e-01,\n",
      "           -1.6955e+00, -1.4717e+00, -1.2889e+00,  1.3605e+00, -2.9553e+00,\n",
      "           -6.2192e-01, -1.4261e+00, -3.1927e-01, -1.2106e+00, -8.3763e-01,\n",
      "            1.6213e+00, -5.2387e-01, -1.0802e+00,  1.0459e+00, -2.7896e-01,\n",
      "            1.9716e+00,  4.4354e-01,  1.8443e-01],\n",
      "          [-6.1496e-01, -1.0053e+00, -3.3617e-01,  1.6575e-01, -3.6331e-01,\n",
      "           -1.6107e+00,  1.0878e+00, -4.7521e-01, -7.1103e-02,  2.8281e-02,\n",
      "           -4.5766e-01,  5.6091e-02,  1.2019e+00, -8.0396e-01, -3.1239e-01,\n",
      "           -3.5686e-01,  1.9468e+00, -9.0210e-01,  7.0454e-01, -8.3644e-01,\n",
      "            9.4192e-01,  1.0121e+00,  7.5070e-01,  7.1520e-01,  1.2997e+00,\n",
      "           -7.3003e-01,  7.1389e-01,  6.6855e-01],\n",
      "          [ 2.0368e+00,  2.5810e-01, -5.0800e-01,  7.8975e-01,  1.3367e+00,\n",
      "           -9.9250e-01,  1.0858e+00,  7.2079e-01,  1.6179e+00,  2.1494e-03,\n",
      "            1.4019e+00,  1.7534e+00, -4.8152e-01,  6.8163e-01, -2.2379e+00,\n",
      "            2.3601e+00,  1.1090e+00, -1.3455e+00,  9.7466e-02,  2.0635e+00,\n",
      "           -1.9481e-01, -3.7672e-01,  7.2143e-01, -2.9006e-01,  3.1841e-01,\n",
      "           -7.6007e-01, -5.9059e-01,  3.9453e-01],\n",
      "          [-2.2662e+00,  5.3662e-01, -1.0635e+00, -3.8428e-01, -1.7349e+00,\n",
      "            7.8727e-01,  6.7702e-02, -1.7440e-01, -7.8281e-01, -8.7509e-01,\n",
      "           -1.6057e+00, -4.8342e-01,  1.0176e+00,  2.4686e-01,  2.2268e+00,\n",
      "           -8.3665e-01,  8.8116e-01,  5.6722e-01, -2.1855e+00, -6.4817e-01,\n",
      "            1.1185e+00,  9.0776e-02,  9.0125e-01, -1.2063e-03, -1.0801e-01,\n",
      "            1.6147e-01, -1.0486e-01,  5.6120e-01]]]])]\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "\n",
    "import onnxruntime\n",
    "\n",
    "onnx_input = [torch_input]\n",
    "print(f\"Input length: {len(onnx_input)}\")\n",
    "print(f\"Sample input: {onnx_input}\")\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"./model.onnx\", providers=['CPUExecutionProvider'])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "onnxruntime_input = {k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)}\n",
    "\n",
    "# onnxruntime returns a list of outputs\n",
    "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch and ONNX Runtime output matched!\n",
      "Output length: 1\n",
      "Sample output: [[ -9.016034  -11.120109    2.1615992  -1.6675048 -21.911633   -3.6054754\n",
      "  -17.77668    -3.433702    6.6276264  -5.816256 ]]\n"
     ]
    }
   ],
   "source": [
    "torch_outputs = cnn(torch_input)\n",
    "\n",
    "assert len(torch_outputs) == len(onnxruntime_outputs)\n",
    "for torch_output, onnxruntime_output in zip(torch_outputs, onnxruntime_outputs):\n",
    "    torch.testing.assert_close(torch_output, torch.tensor(onnxruntime_output))\n",
    "\n",
    "print(\"PyTorch and ONNX Runtime output matched!\")\n",
    "print(f\"Output length: {len(onnxruntime_outputs)}\")\n",
    "print(f\"Sample output: {onnxruntime_outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
